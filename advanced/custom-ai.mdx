---
title: 'AI Models'
description: 'Your guide to installing the best model for your local AI'
icon: 'robot'
---

<Tip>
AI models in Shinkai are powered by Ollama and are constantly being updated.
</Tip>


## Understanding AI Models
If you're looking to install an AI model but aren’t sure where to start, understanding the key features of each model can make your decision easier. 
Factors like the number of **parameters** and **quantization** levels help determine which model suits your needs. 

Whether you’re using it for casual text generation or advanced AI tasks, learning these basics ensures you pick the right model for your device and tasks. 
**Let's break down these features to help guide your choice**.

<ResponseField name="Parameters" type="e.g., 8B, 12B, 70B">
  <Expandable title="Parameters definition">
    **Parameters** refer to the model's size in terms of billions of parameteers. These are the internal settings of the model that are adjusted during training, determining how well it can learn, predict, generate text, or recognize patterns. 
    **The more parameters a model has, the more detailed and powerful it can be**, though it also requires more computing power. Some examples:

    <ResponseField name="8B Parameters" type="e.g., LLaMA 8B">
        - **Uses**: Suitable for tasks like general text generation, chatbots, and lightweight language understanding.
        - **System Requirements**: Requires around 16GB of RAM and an 8-core CPU. A GPU is optional but improves performance.
    </ResponseField>
    <ResponseField name="12B Parameters" type="e.g., Mistral-Nemo 12B">
        - **Uses**: Best for longer-context tasks, such as multi-lingual support and more complex text generation.
        - **System Requirements**: Needs at least 16GB of RAM and a strong CPU; GPU is recommended for better speed.
    </ResponseField>
    <ResponseField name="70B Parameters" type="e.g., LLaMA 70B">
        - **Uses**: Ideal for large-scale tasks like data analysis, advanced text generation, and more sophisticated AI tools.
        - **System Requirements**: Requires 32GB RAM, high-end CPUs, and a GPU for optimal performance due to its complexity.    
    </ResponseField>
  </Expandable>
</ResponseField>

<ResponseField name="Quantization" type="e.g., Q4, Q8, FP16">
  <Expandable title="Quantization definition">
   **Quantization** is a technique used to reduce the size of AI models by simplifying the number of bits used to represent the parameters. 
   This makes the model faster and more efficient without drastically losing accuracy. Here are some examples:

    <ResponseField name="Q4" type="4-bit Quantization">
        This reduces memory usage and speeds up computation, making it **ideal for devices with limited resources** or when model size needs to be minimized. 
        Choose this when you need efficiency over slight reductions in accuracy.
    </ResponseField>
    <ResponseField name="8Q" type="8-bit Quantization">
        A **balance between performance and precision**. It’s suitable for most tasks where some compression is acceptable but higher accuracy is still required.
    </ResponseField>
    <ResponseField name="FP16" type="16-bit Floating Point">
        Maintains high accuracy while reducing model size compared to FP32. **Best for tasks where accuracy is more important** but some efficiency is needed.  
    </ResponseField>
  </Expandable>
</ResponseField>

### AI Models Comparison
Choosing the right AI model depends on the task you want to perform and the specs of your device. 
Models may have different versions to install, here are a few examples:
| **Model**          | **Parameters**                     | **Quantization** | **Best For**                                |
|--------------------|------------------------------------|------------------|---------------------------------------------|
| **Llama3.1**       | 8B, 70B, 405B                     | Q8, FP16         | Advanced tools, large-scale tasks           |
| **Gemma2**         | 2B, 9B, 27B                       | Q8               | Efficient text generation and language tasks|
| **Mistral-Nemo**   | 12B, 70B                          | Q4               | Long-context tasks, multi-lingual support   |
| **Qwen2**          | 0.5B, 1.5B, 7B, 72B               | Q4, Q8           | Text processing, general AI tasks           |
| **Deepseek-Coder** | 16B, 236B                         | Q8, FP16         | Code generation, fill-in-the-middle tasks   |
| **CodeGemma**      | 2B, 7B                            | Q8               | Code generation, instruction-following tasks|

### Browsing AI models in Shinkai

Shinkai offers you a wide array of models to choose from. To browse the whole list of AI models in Shinkai, click the `AIs` option of the left side menu > Click the `Show all models` option below the recommended models. 

<Frame>
  <img src="/images/models-list.jpg" style={{ borderRadius: '0.5rem' }} />
</Frame>
In this view you can access the different models available in Shinkai. This list is continously being updated. 

<Tip>
    You can choose the version of the model you'd like to install in the dropdown menu next to the model's name. 
    
    Refer to the **[parameters and quantization](/advanced/custom-ai#understanding-ai-models)** guidelines to help you make the best choice for your device and task. 
</Tip>

## Installing AIs Manually
The list of AI models in Shinkai is powered by Ollama and is continously growing. Although, if you want to install another model, you can do it easily:

1.  Click the `AIs` option on the left side menu > Click the `+ Manually Add AI` button on the upper-right corner.
<Frame>
  <img src="/images/manual-ai.jpg" style={{ borderRadius: '0.5rem' }} />
</Frame>
2. Select the model you'd like to install. Currently, the available options are: *OpenAI, Together AI, Ollama, Gemini, and Exo*.
3. Choose the model type or version you want to install. 
4. Customize the name of the model or leave the name given by default.
5. Enter the URL of the model source. 
6. Enter the **API key** for the model *(e.g., you need an active subscription in OpenAI to request API keys of their models)*. 
7. Click 'Add AI'. You will now see the model available in the `AIs` view and within the model dropdown of AI Chats.  


<Frame>
  <img src="/images/manual-ai-b.jpg" style={{ borderRadius: '0.5rem' }} />
</Frame>

### Custom AIs
If the model you want to install is not present in the models list, you can add custom models providing the information requested in the form: *Model name (service provider); model type; model name to appear in the chats; external URL; and API key*. 

<Frame>
  <img src="/images/custom-ai.jpg" style={{ borderRadius: '0.5rem' }} />
</Frame>
